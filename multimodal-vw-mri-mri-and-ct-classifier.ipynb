{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import monai\nimport torch\nimport os\nimport pandas as pd\nimport torch.nn as nn\nfrom monai.transforms import (\n    Compose, LoadImage, AddChannel, ScaleIntensity, ToTensor, Resize)\nfrom monai.data import Dataset, DataLoader\nfrom monai.networks.nets import DenseNet121\nfrom torch.utils.data import random_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 1. Data Preprocessing\n","metadata":{}},{"cell_type":"code","source":"vw_mri_dir = \"/data/vw_mri/\"\nregular_mri_dir = \"/data/regular_mri/\"\nct_dir = \"/data/ct_scans/\"  \n\nlabels_file = \"/data/labels.csv\"\nlabels_df = pd.read_csv(labels_file)\n\nvw_mri_paths = [os.path.join(vw_mri_dir, f\"{row['filename']}\") for _, row in labels_df.iterrows()]\nregular_mri_paths = [os.path.join(regular_mri_dir, f\"{row['filename']}\") for _, row in labels_df.iterrows()]\nct_paths = [os.path.join(ct_dir, f\"{row['filename']}\") for _, row in labels_df.iterrows()]\n\nlabels = labels_df['label'].tolist()\n\ndata_dicts = [{\"vw_mri\": vw, \"regular_mri\": reg, \"ct\": ct, \"label\": label}\n              for vw, reg, ct, label in zip(vw_mri_paths, regular_mri_paths, ct_paths, labels)]\n\ntransform_vw_mri_3d = Compose([\n    LoadImage(image_only=True),\n    AddChannel(),\n    ScaleIntensity(),\n    Resize(spatial_size=(128, 128, 64)),  \n    ToTensor()\n])\n\ntransform_2d = Compose([\n    LoadImage(image_only=True),\n    AddChannel(),\n    ScaleIntensity(),\n    Resize(spatial_size=(128, 128)),  \n    ToTensor()\n])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Making Multimodal Dataset","metadata":{}},{"cell_type":"code","source":"class MultimodalDataset(Dataset):\n    def __init__(self, data, transform_vw_mri_3d, transform_2d):\n        self.data = data\n        self.transform_vw_mri_3d = transform_vw_mri_3d\n        self.transform_2d = transform_2d\n\n    def __getitem__(self, index):\n        item = self.data[index]\n        vw_mri = self.transform_vw_mri_3d(item[\"vw_mri\"])\n        regular_mri = self.transform_2d(item[\"regular_mri\"])\n        ct = self.transform_2d(item[\"ct\"])\n        label = torch.tensor(item[\"label\"], dtype=torch.long)\n        return vw_mri, regular_mri, ct, label\n\n    def __len__(self):\n        return len(self.data)\n\ndataset = MultimodalDataset(data_dicts, transform_vw_mri_3d, transform_2d)\ntrain_set, val_set = random_split(dataset, [int(len(dataset) * 0.8), int(len(dataset) * 0.2)])\ntrain_loader = DataLoader(train_set, batch_size=2, shuffle=True)\nval_loader = DataLoader(val_set, batch_size=2)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Designing Multimodal Network for both Modalities","metadata":{}},{"cell_type":"code","source":"class MultimodalNetwork3D2D(nn.Module):\n    def __init__(self):\n        super(MultimodalNetwork3D2D, self).__init__()\n        \n        self.vw_mri_net_3d = DenseNet121(spatial_dims=3, in_channels=1, out_channels=2)\n        \n        self.regular_mri_net_2d = DenseNet121(spatial_dims=2, in_channels=1, out_channels=2)\n        \n        self.ct_net_2d = DenseNet121(spatial_dims=2, in_channels=1, out_channels=2)\n        \n        self.fc1 = nn.Linear(6, 512)  \n        self.fc2 = nn.Linear(512, 256)  \n        self.fc3 = nn.Linear(256, 128)  \n        self.fc4 = nn.Linear(128, 2)    \n        \n        self.dropout = nn.Dropout(p=0.5)  \n\n    def forward(self, vw_mri, regular_mri, ct):\n        vw_out = self.vw_mri_net_3d(vw_mri)  \n        regular_out = self.regular_mri_net_2d(regular_mri) \n        ct_out = self.ct_net_2d(ct)  \n        \n        combined = torch.cat((vw_out, regular_out, ct_out), dim=1)\n        \n        x = torch.relu(self.fc1(combined))\n        x = self.dropout(x)\n        x = torch.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = torch.relu(self.fc3(x))\n        x = self.dropout(x)\n        x = self.fc4(x)  \n        \n        return x\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = MultimodalNetwork3D2D().to(device)\nloss_function = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training Model and Evaluating on Validation Set","metadata":{}},{"cell_type":"code","source":"def train_model(num_epochs, model, train_loader, val_loader, optimizer, loss_function, device):\n    for epoch in range(num_epochs):\n        model.train()\n        epoch_loss = 0\n        for vw_mri, regular_mri, ct, labels in train_loader:\n            vw_mri, regular_mri, ct, labels = vw_mri.to(device), regular_mri.to(device), ct.to(device), labels.to(device)\n            \n            outputs = model(vw_mri, regular_mri, ct)\n            loss = loss_function(outputs, labels)\n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        print(f\"Epoch {epoch+1}, Loss: {epoch_loss/len(train_loader)}\")\n        \n        model.eval()\n        val_loss = 0\n        correct = 0\n        with torch.no_grad():\n            for vw_mri, regular_mri, ct, labels in val_loader:\n                vw_mri, regular_mri, ct, labels = vw_mri.to(device), regular_mri.to(device), ct.to(device), labels.to(device)\n                outputs = model(vw_mri, regular_mri, ct)\n                loss = loss_function(outputs, labels)\n                val_loss += loss.item()\n                \n                _, predicted = torch.max(outputs, 1)\n                correct += (predicted == labels).sum().item()\n        \n        val_loss /= len(val_loader)\n        accuracy = correct / len(val_loader.dataset)\n        print(f\"Validation Loss: {val_loss}, Accuracy: {accuracy}\")\n\ntrain_model(20, model, train_loader, val_loader, optimizer, loss_function, device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}